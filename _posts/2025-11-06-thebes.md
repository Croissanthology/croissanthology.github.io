---
layout: post
title: "Thebes art "
date: 2025-11-06
permalink: /thebes
status: "in progress"
importance: 4
redirect_from:
  - /thebes-art
  - /thebes-art/
  - /vogel
  - /vogel/
  - /voooooogel
  - /voooooogel/
---

The way the world is formed, *anything* can be interesting enough to write at length about. Humans have a [test-time-compute](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) and general ability[^1] to do what large language models at the moment [seemingly cannot](https://gwern.net/ai-daydreaming), allowing for countless variations between subjects, ideas, footnotes, nested footnotes, alt text, writing, drawings, data, anything, really.


## Lorem heading






![Tweet by Gavin Leech, infamous PhD and YouTuber, commenting humorously on a METR eval chart itself commented by Toby Ord of the now-extinct Future of Humanity Institute of Oxford. The METR eval chart shows how for every marginal hour of work, different models + humans (represented by a gray line) improve their “average normalized score” at whatever this benchmark arena is. Gavin says, britishly, "this new grey model has crazy-good test-time scaling. Should keep an eye on it”. Meanwhile, rising twitter picocelebrity croissanthology dot com comments “how far along do you reckon it rises till it essentially plateaus". Gavin, inspired by a new streak of wit, parries with a single photo of 4 books in Robert A. Caro's magnum opus biography of Lyndon B. Johnson. The point here is that humans, given enough time, tenacity, intelligence, can do practically anything cf. e.g. David Deutsche: in this case, becoming president of the United States. Can large language models do the same? The world can only await with baited breath. It does not yet seem like this is the case.](https://i.imgur.com/VivSqXa.png)





---

[^1]: See Gavin Leech:
